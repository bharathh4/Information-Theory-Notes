import math
def H(probs):
    _H = -sum([prob * math.log(prob, 2) for prob in probs])
    return _H
if __name__ == "__main__":
    probs = [0.999, 0.0005, 0.0005]
    print(H(probs))
    probs = [0.33, 0.33, 0.33]
    print(H(probs))

One of the way of thinking about H(X) is the average number of steps or questions
to ask before you know the answer to a question (is it a car, bird or tree)
It does make sense to ask questions about highly probable items first and 
the low probability ones later. So a low probable item is down in the tree. 
This means a suitable metric should relect the high number of steps it takes 
to reach a low probability item. Something like -log2(pron). 

-log2(1.0) = 0
-log2(0.0009765625) = 10

H(X) is the basic quantity in information theory. In an important sense, all 
the other quantities we’ll compute are variant on it. H(X) goes by a number of 
different names: “uncertainty”, “information”, even “entropy” (a term from 
the physical sciences, which we’ll return to later). It has units called bits.

Say in the game the likelihood of all item is equally likely ie 
P(car) = 0.33
P(tree) = 0.33
P(bird) = 0.33

Then H = 1.58
You need to ask 2 question or use 2 bits to represent this data

If 

P(car) = 1
P(tree) = 0
P(bird) = 0

Then you know you don't need to ask any questions because no matter 
what question you ask the answer is always a 'car'.  
Also H = 0 for the case. So the quantity agress that you we needn't 
represent it or ask any questions.

So a highly skewd distribution has low information and a uniform 
distribution has high information, that is, it takes more bits
to describe the data.


When the tree is optimal, the encoding is efficient. Computer 
scientists recognize the construction of the optimal tree as 
Huffman Encoding. Huffman Encoding is the most basic form of lossless
compression. Huffman Encoding is an incredibly simple way to achieve optimal 
transmission and storage, and it’s therefore widespread in the computer 
world. When you “zip” something, the algorithm under the hood is very often 
some variant of Huffman Encoding. It’s a lossless encoding, meaning that
even though it generally finds a much faster way to transmit the information,
nothing is lost. It just takes advantage of the biases in the source, giving
nicknames or shortcuts to the most common things you say


Shannon's introduction

Shannon built up his story axiomatically, by saying that he wanted to measure the uncertainty
in a process. We want a function, in other words, call it H(~p), that takes a list of probabilities and
spits out a single number, uncertainty. Let’s require the function to obey four simple axioms.

1. Continuity (if I only change the probabilities a little, the information of the process should
change only a little).
2. Symmetry (if I reorder the list of probabilities I gave you, you should get the same answer).
3. Condition of Maximum Information: H(~p) is at its maximum value when all the pi are equal.
4. Coarse-Graining. The Coarse-Graining axiom takes the idea of uncertainty step further. It’s really about how
we group things together and throw out distinctions. You can lump two variables 


Kullback-Leibler Divergence
We can describe KL in terms of question script inefficiencies, or coding failures, and like uncertainty itself, there are many interpretations beyond that. We can talk, for example, about the
“surprise” (or “Bayesian surprise”) of one distribution given that you’re expecting another

The form is very similar to H(X)

For these latter reasons, minimizing the KL of a distribution p from the “true” distribution q
can be understood as making p as epistemically close to truth as possible; hence its use in model selection, where it is part of the Akaike Information Criterion [3], as well as its role in our new
proposal for correcting algorithmic bias when machine learning is used for criminal justice and
policy-making [4].

People sometimes use KL as a distance measure—“how different are two distributions”—but this
is incorrect for a very interesting reason: KL is not necessarily symmetric! The surprise on encountering q given that you’re expecting p may not be the surprise on encountering p given that you’re
expecting q. An organism that grows up in a rich sensory environment will, in general, be less surprised on encountering an improvised one than an organism who has grown up in an impoverished
environment and encounters a rich one.

(Because Kullback-Leibler divergence is not symmetric, some people like to invent a distance
measure for the “distance” between p and q by symmetrizing it: KL(p|q) + KL(q|p). This is a
suboptimal solution because, while it does make the measure symmetric, it still fails as a measure
of distance because it violates the triangle inequality, which requires that the distance from p to q
must be less than or at worst equal to the distance from p to r plus the distance from r to q. Below,
we’ll see a better metric to use in its place.)

Mutual Information
Once you can measure H(X), the uncertainty in a process, you can talk about how that uncertainty
changes when you acquire information from somewhere else. For example, while you might be
maximally uncertain about the card on the top of a shuffled deck, your uncertainty goes down a
great deal once I tell you the suit. Instead of having an equal choice over 52 cards (an entropy of
log2 52, about 5.7 bits), you have an equal choice over the thirteen cards of that suit (log2 13, or
3.7, bits). Put another way, telling you the suit was worth two bits of information

In general, if two variables, X and Y , are correlated with each other, this means that the
probability distribution over one of the variables is affected by the value of the other variable. If it’s
raining, then I’m more likely to be carrying an umbrella;

If we average over all the possible values of Y , weighted by their probability, we get what is
called the conditional entropy of X given Y , or H(X|Y )

H(X|y) can be larger than
H(X); if, for example, X gets more random under unusual conditions. However, it can be shown
(using something called Jensen’s inequality) that on average, information never hurts. In other
words, H(X|Y ) is always less than, or (in the case that Y and X are independent) equal to H(X).
We usually talk in terms of the extent to which Y reduces uncertainty about X; the mutual
information, I, between X and Y is thus defined as
I(X, Y ) = H(X) − H(X|Y )

In an analogous fashion to how uncertainty
talks about the optimal question script (without needing to specify it in details), mutual information
talks about the optimal method for learning about one thing using information about another.


Mutual Information is at the heart of a number of problems where people try to figure out how
much information, or influence, is propagating between systems

Jensen-Shannon Distance
Let’s return to Mom, Dad, and twenty questions one last time. Say you’re playing over a computer
with one of them, but you don’t know which one. Over time, as you play, you will start to accumulate
information in favor of one or the other—you’ll notice a preponderance of “car” choices, indicating
your father, or “tree” choices, indicating your mother.
How much information about the identity of the player do you get from playing a single round?
From the symmetry of mutual of information, this is equivalent to how much information about
the identity of the word you get from learning who the player is, or how much your uncertainty is
reduced, on average, by gaining this. This is called the Jensen-Shannon Distance, or JSD, between
P and Q:
JSD(P, Q) = H(M) − 1/2  * (H(P) + H(Q))

where M is the mixture distribution of the two adults, mi
is equal to 1
2
(pi + qi). In words, the
JSD tells you how much one sample, on average serves to distinguish between the two possibilities.
Once you know which distribution you’re drawing from you will (on average) be able to construct
a more efficient question tree. The unit of JSD is (of course) bits—the number of yes/no questions
you get for free (again, on average) once you know which of the two distributions, P or Q, you’ll
be drawing words from.
A simple computation shows that this is equal to the KL from M to P plus the KL from M to
Q,
JSD(P, Q) = 1/2 * (KL(M|P) + KL(M|Q))

For these reasons, JSD serves well as a method for understanding distinctiveness a “distance”
between two probability distributions. It plays the same role for probabilities as Euclidean distance
does for ordinary “spatial” vectors.

If you have enough data, and not too many categories, this will usually work very well. A
good rule of thumb is that you should have at least ten times as many samples as you do classes or
categories that you’re measuring [11]. So, twenty tosses of a coin are sufficient to get a good estimate
of the uncertainty of the distribution; forty samples of two stocks will be enough to determine the
mutual information between their “up” or “down” ticks

When you don’t have enough data—or you want to do more sophisticated things, like determine
error bars on estimates—you have to be a bit smarter. 

